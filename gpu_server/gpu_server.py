import asyncio, datetime as dt, os, shutil, subprocess, tarfile, uuid
from pathlib import Path
from typing import Dict, Literal, Optional

import requests
from fastapi import BackgroundTasks, FastAPI, HTTPException
from pydantic import BaseModel, Field

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ĞšĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ñ‹ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞµÑ€Ğ²ĞµÑ€Ğ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_SERVER   = "http://msi.lan:8000"
LIST_URL      = f"{DATA_SERVER}/list"      # GET  â†’ {"files": ["rbs_ros2bag", ...]}
DOWNLOAD_URL  = f"{DATA_SERVER}/download"  # GET  dataset_name+".zip"
UPLOAD_URL    = f"{DATA_SERVER}/upload"    # POST multipart/form-data

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ GPUâ€‘ÑƒĞ·Ğ»Ğ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_DIR = Path("data")   # <datasets>/<dataset_name>/â€¦
JOBS_DIR = Path("jobs")       # <jobs>/<job_id>/{output,train.log,model.tar.gz}
DATA_DIR.mkdir(exist_ok=True)
JOBS_DIR.mkdir(exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Pydanticâ€‘Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class TrainRequest(BaseModel):
    dataset_name : str
    steps        : int  = 100_000
    device       : Literal["cpu", "cuda"] = "cuda"
    repo_id      : str  = "rbs_ros2bag"
    use_vae      : bool = True
    root_override: Optional[str] = None

class JobStatus(BaseModel):
    job_id    : str
    state     : Literal["pending", "running", "finished", "failed"] = "pending"
    created_at: dt.datetime
    updated_at: dt.datetime
    progress  : Optional[float] = None   # 0â€“100
    message   : Optional[str] = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Runtimeâ€‘storage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
JOBS: Dict[str, JobStatus] = {}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FastAPI app â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = FastAPI(title="GPU Training Orchestrator")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ’ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _update_job(job_id: str, **kw):
    job = JOBS[job_id]
    for k, v in kw.items(): setattr(job, k, v)
    job.updated_at = dt.datetime.utcnow()

def download_dataset(relative_path: Path):
    """"""
    save_path = Path(DATA_DIR) / relative_path
    save_path.parent.mkdir(parents=True, exist_ok=True)

    print(f"â¬‡ï¸  Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ°: {relative_path}")
    try:
        response = requests.get(DOWNLOAD_URL, params={"filename": str(relative_path)}, stream=True)
        response.raise_for_status()
        with open(save_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        print(f"âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾: {save_path}")
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ {relative_path}: {str(e)}")

def upload_weights(job_id: str, output_dir: Path):
    """ĞÑ€Ñ…Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµĞ¼ output_dir Ğ¸ Ğ¾Ñ‚ÑÑ‹Ğ»Ğ°ĞµĞ¼ Ğ½Ğ° Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞµÑ€Ğ²ĞµÑ€."""
    tar_path = output_dir.with_suffix(".tar.gz")
    with tarfile.open(tar_path, "w:gz") as tar:
        tar.add(output_dir, arcname=".")
    with open(tar_path, "rb") as f:
        r = requests.post(UPLOAD_URL, files={"file": (tar_path.name, f)}, timeout=30)
        r.raise_for_status()                # 4xx/5xx => Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ
    tar_path.unlink()                       # ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ°Ñ€Ñ…Ğ¸Ğ²

async def run_training(files: list[Path], job_id: str, req: TrainRequest):
    job_root   = JOBS_DIR / job_id
    log_path   = job_root / "train.log"
    output_dir = job_root / "output"
    job_root.mkdir(parents=True, exist_ok=True)

    # ---------- helper Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ² Ğ»Ğ¾Ğ³ ----------
    def write_log(msg: str):
        ts = dt.datetime.utcnow().isoformat(sep=" ", timespec="seconds")
        with open(log_path, "a") as f:
            f.write(f"[{ts}] {msg}\n")
            f.flush()

    try:
        # 1. Dataset
        local_ds = DATA_DIR / req.dataset_name
        if not local_ds.exists():
            _update_job(job_id, message="Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚â€¦")
            write_log("Downloading dataset")
            for file_path in files:
                download_dataset(req.dataset_name + "/" + file_path)

        # 2. ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
        cmd = [
            "python", "-m", "lerobot.scripts.train",
            "--dataset.repo_id", req.repo_id,
            "--dataset.root", req.root_override or str(local_ds),
            "--policy.type=act",
            f"--output_dir={output_dir}",
            f"--job_name={job_id}",
            f"--policy.device={req.device}",
            f"--policy.use_vae={str(req.use_vae).lower()}",
            f"--steps={req.steps}",
            f"--policy.repo_id={req.repo_id}",
        ]
        write_log("CMD: " + " ".join(cmd))

        # 3. Ğ—Ğ°Ğ¿ÑƒÑĞº
        _update_job(job_id, state="running", message="Ğ—Ğ°Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ")
        proc = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.STDOUT,   # Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ stderr Ğ² stdout
        )

        async for raw in proc.stdout:           # raw = bytes
            line = raw.decode("utf-8", errors="replace").rstrip()
            write_log(line)
            if "step" in line and "/" in line:
                try:
                    cur, total = map(int, line.split("step")[1].split()[0].split("/"))
                    _update_job(job_id, progress=cur * 100 / total)
                except Exception:
                    pass

        rc = await proc.wait()
        if rc != 0:
            raise RuntimeError(f"ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ²ĞµÑ€Ğ½ÑƒĞ» ĞºĞ¾Ğ´ {rc}")

        # 5. Ğ’Ñ‹Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ²ĞµÑĞ¾Ğ² âŸ¶ NAS
        _update_job(job_id, message="Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ²ĞµÑĞ° Ğ½Ğ° NASâ€¦", progress=100.0)
        write_log("Uploading weights to NAS")
        upload_weights(job_id, output_dir)

        # 6. ĞÑ‡Ğ¸ÑÑ‚ĞºĞ°
        shutil.rmtree(local_ds,  ignore_errors=True)
        shutil.rmtree(output_dir, ignore_errors=True)
        write_log("Cleanup complete")

        _update_job(job_id, state="finished", message="Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾. Ğ’ĞµÑĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ñ‹")

    except Exception as e:
        write_log(f"ERROR: {e}")
        _update_job(job_id, state="failed", message=str(e))


def get_dataset(name:str):
    print("ğŸ“¥ Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ ÑĞ¿Ğ¸ÑĞºĞ° Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ñ ÑĞµÑ€Ğ²ĞµÑ€Ğ°...")
    try:
        response = requests.get(LIST_URL, params={"name": name})
        response.raise_for_status()
        return response.json().get("files", [])
    except Exception as e:
        print("âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¸ÑĞºĞ°:", str(e))
        return []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.post("/train")
async def train(req: TrainRequest, bg: BackgroundTasks):
    # Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ½Ğ° NAS
    files = get_dataset(req.dataset_name)
    if not files:
        raise HTTPException(404, "Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ½Ğ° NAS")

    job_id = uuid.uuid4().hex[:8]
    JOBS[job_id] = JobStatus(job_id=job_id, created_at=dt.datetime.utcnow(), updated_at=dt.datetime.utcnow())
    bg.add_task(run_training, files, job_id, req)
    return {"job_id": job_id, "status_url": f"/status/{job_id}"}

@app.get("/status/{job_id}")
def status(job_id: str):
    job = JOBS.get(job_id) or HTTPException(404, "Job not found")
    return job

@app.get("/log/{job_id}")
def log(job_id: str, lines: int = 50):
    lp = JOBS_DIR / job_id / "train.log"
    if not lp.exists(): raise HTTPException(404, "Ğ›Ğ¾Ğ³ ĞµÑ‰Ñ‘ Ğ½Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½")
    return {"log_tail": "".join(lp.read_text().splitlines()[-lines:])}

@app.get("/")
def root(): return {"message": "GPU ÑĞµÑ€Ğ²ĞµÑ€ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½"}
